# ============================================================================
# LLM Configuration (Phase 2)
# ============================================================================

# LLM Provider: ollama, openai, anthropic, azure
LLM_PROVIDER=ollama

# Local Models (Ollama) - Choose one:
# - deepseek-coder-v2:16b (best quality, needs 20GB RAM)
# - qwen2.5-coder:14b (balanced, needs 18GB RAM) 
# - codellama:13b (fast, needs 16GB RAM)
# - codellama:7b (lightweight, needs 8GB RAM)
LLM_MODEL=qwen2.5-coder:14b

# Sampling controls
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=4096

# Ollama Server URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Internal URL for containers (default points to dockerized service)
OLLAMA_INTERNAL_URL=http://ollama:11434
# Optional port override if you already use 11434
OLLAMA_PORT=11434

# Cloud API Keys (optional, only if using cloud providers)
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here

# Custom endpoints (override if needed)
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1

# Azure OpenAI (optional)
AZURE_OPENAI_API_KEY=your_azure_key_here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/

# Fallback to local model if cloud fails
LLM_FALLBACK_TO_LOCAL=true

# ============================================================================
# ESP-IDF Configuration
# ============================================================================

# Target chip: esp32, esp32s2, esp32s3, esp32c3, esp32c6, esp32h2
ESP_IDF_TARGET=esp32c6

# Serial port for flashing (adjust for your system)
# macOS: /dev/cu.usbmodem* or /dev/cu.usbserial*
# Linux: /dev/ttyUSB0 or /dev/ttyACM0
SERIAL_PORT=/dev/cu.usbmodem21101

# ============================================================================
# Optional Settings
# ============================================================================

# QEMU timeout (seconds)
QEMU_TIMEOUT=300

# Enable verbose logging
DEBUG=false
