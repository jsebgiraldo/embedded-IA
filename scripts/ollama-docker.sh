#!/bin/bash

# Script para gestionar Ollama en Docker
# Uso: ./scripts/ollama-docker.sh [start|stop|status|pull|run|logs]

set -e

CONTAINER_NAME="esp32-ollama"
MODEL_NAME="${LLM_MODEL:-qwen2.5-coder:14b}"

# Colores
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

print_header() {
    echo -e "${BLUE}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó${NC}"
    echo -e "${BLUE}‚ïë          Ollama Docker Manager - ESP32 DevAgent             ‚ïë${NC}"
    echo -e "${BLUE}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù${NC}"
    echo ""
}

# Verificar si el contenedor existe
container_exists() {
    docker ps -a --format '{{.Names}}' | grep -q "^${CONTAINER_NAME}$"
}

# Verificar si el contenedor est√° corriendo
container_running() {
    docker ps --format '{{.Names}}' | grep -q "^${CONTAINER_NAME}$"
}

# Iniciar servicios
start_ollama() {
    print_header
    echo -e "${YELLOW}üöÄ Iniciando Ollama en Docker...${NC}"
    
    # Iniciar compose (solo ollama)
    docker-compose up -d ollama
    
    echo -e "${GREEN}‚úÖ Esperando a que Ollama est√© listo...${NC}"
    sleep 5
    
    # Verificar health
    for i in {1..30}; do
        if docker exec $CONTAINER_NAME ollama list > /dev/null 2>&1; then
            echo -e "${GREEN}‚úÖ Ollama est√° listo!${NC}"
            show_status
            return 0
        fi
        echo -n "."
        sleep 1
    done
    
    echo -e "${RED}‚ùå Timeout esperando a Ollama${NC}"
    return 1
}

# Detener Ollama
stop_ollama() {
    print_header
    echo -e "${YELLOW}üõë Deteniendo Ollama...${NC}"
    
    if container_running; then
        docker-compose stop ollama
        echo -e "${GREEN}‚úÖ Ollama detenido${NC}"
    else
        echo -e "${YELLOW}‚ö†Ô∏è  Ollama no estaba corriendo${NC}"
    fi
}

# Mostrar estado
show_status() {
    print_header
    
    if container_running; then
        echo -e "${GREEN}‚úÖ Estado: CORRIENDO${NC}"
        echo ""
        
        # Stats del contenedor
        echo -e "${BLUE}üìä Recursos:${NC}"
        docker stats --no-stream --format "  CPU: {{.CPUPerc}}\n  RAM: {{.MemUsage}}" $CONTAINER_NAME
        echo ""
        
        # Modelos disponibles
        echo -e "${BLUE}ü§ñ Modelos disponibles:${NC}"
        docker exec $CONTAINER_NAME ollama list 2>/dev/null || echo "  (ninguno instalado)"
        echo ""
        
        # Endpoint
        echo -e "${BLUE}üåê Endpoint:${NC}"
        echo "  http://localhost:11434"
        echo ""
    else
        echo -e "${RED}‚ùå Estado: DETENIDO${NC}"
        echo ""
        echo "Usa: $0 start"
    fi
}

# Descargar modelo
pull_model() {
    print_header
    
    if ! container_running; then
        echo -e "${RED}‚ùå Ollama no est√° corriendo${NC}"
        echo "Usa: $0 start"
        exit 1
    fi
    
    MODEL="${1:-$MODEL_NAME}"
    
    echo -e "${YELLOW}üì• Descargando modelo: ${MODEL}${NC}"
    echo ""
    
    docker exec -it $CONTAINER_NAME ollama pull "$MODEL"
    
    echo ""
    echo -e "${GREEN}‚úÖ Modelo descargado!${NC}"
}

# Ejecutar modelo interactivo
run_model() {
    print_header
    
    if ! container_running; then
        echo -e "${RED}‚ùå Ollama no est√° corriendo${NC}"
        echo "Usa: $0 start"
        exit 1
    fi
    
    MODEL="${1:-$MODEL_NAME}"
    
    echo -e "${YELLOW}üí¨ Iniciando chat con: ${MODEL}${NC}"
    echo -e "${BLUE}   (Ctrl+D para salir)${NC}"
    echo ""
    
    docker exec -it $CONTAINER_NAME ollama run "$MODEL"
}

# Ver logs
show_logs() {
    print_header
    echo -e "${BLUE}üìã Logs de Ollama:${NC}"
    echo ""
    docker logs --tail 50 -f $CONTAINER_NAME
}

# Test r√°pido
test_ollama() {
    print_header
    
    if ! container_running; then
        echo -e "${RED}‚ùå Ollama no est√° corriendo${NC}"
        exit 1
    fi
    
    MODEL="${1:-$MODEL_NAME}"
    
    echo -e "${YELLOW}üß™ Test r√°pido con ${MODEL}${NC}"
    echo ""
    
    # Test simple
    PROMPT="Write a hello world in C for ESP32"
    
    echo -e "${BLUE}Prompt:${NC} $PROMPT"
    echo ""
    echo -e "${BLUE}Respuesta:${NC}"
    
    docker exec $CONTAINER_NAME curl -s http://localhost:11434/api/generate -d "{
        \"model\": \"$MODEL\",
        \"prompt\": \"$PROMPT\",
        \"stream\": false
    }" | python3 -c "import sys, json; print(json.load(sys.stdin)['response'])"
    
    echo ""
    echo -e "${GREEN}‚úÖ Test completado${NC}"
}

# Reiniciar
restart_ollama() {
    stop_ollama
    sleep 2
    start_ollama
}

# Men√∫ principal
case "${1:-}" in
    start)
        start_ollama
        ;;
    stop)
        stop_ollama
        ;;
    restart)
        restart_ollama
        ;;
    status)
        show_status
        ;;
    pull)
        pull_model "${2:-}"
        ;;
    run)
        run_model "${2:-}"
        ;;
    logs)
        show_logs
        ;;
    test)
        test_ollama "${2:-}"
        ;;
    *)
        print_header
        echo "Uso: $0 {start|stop|restart|status|pull|run|logs|test} [modelo]"
        echo ""
        echo "Comandos:"
        echo "  start              Iniciar Ollama"
        echo "  stop               Detener Ollama"
        echo "  restart            Reiniciar Ollama"
        echo "  status             Ver estado y recursos"
        echo "  pull [modelo]      Descargar modelo"
        echo "  run [modelo]       Chat interactivo"
        echo "  logs               Ver logs en tiempo real"
        echo "  test [modelo]      Test r√°pido"
        echo ""
        echo "Ejemplos:"
        echo "  $0 start"
        echo "  $0 pull qwen2.5-coder:14b"
        echo "  $0 run qwen2.5-coder:14b"
        echo "  $0 test"
        echo "  $0 status"
        echo ""
        exit 1
        ;;
esac
