# üß† Local LLM Setup Guide

This guide explains how to run powerful AI models locally for the Developer Agent, eliminating dependency on cloud APIs and improving privacy.

---

## üéØ Recommended Local Models for ESP32 Development

### Top 3 Models (Ordered by Power)

| Model | Size | RAM Needed | Speed | Quality | Best For |
|-------|------|------------|-------|---------|----------|
| **DeepSeek Coder V2 16B** | 16B | 16-20GB | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Complex debugging, architectural changes |
| **Qwen2.5-Coder 14B** | 14B | 14-18GB | Fast | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ESP32 code, embedded systems |
| **CodeLlama 13B** | 13B | 12-16GB | Fast | ‚≠ê‚≠ê‚≠ê‚≠ê | Quick fixes, code completion |

### Alternative Options

| Model | Size | RAM Needed | Speed | Quality | Best For |
|-------|------|------------|-------|---------|----------|
| **Llama 3.1 8B Instruct** | 8B | 8-10GB | Very Fast | ‚≠ê‚≠ê‚≠ê‚≠ê | General coding, fast responses |
| **CodeLlama 7B** | 7B | 6-8GB | Very Fast | ‚≠ê‚≠ê‚≠ê | Lightweight, older Macs |
| **Phi-3 Medium** | 14B | 14GB | Fast | ‚≠ê‚≠ê‚≠ê‚≠ê | Good balance, Microsoft |

---

## üöÄ Option 1: Ollama (Recommended - Easiest)

**Best for**: Beginners, macOS users, quick setup

### Installation

```bash
# macOS (via Homebrew)
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Start Ollama server
ollama serve
```

### Download Models

```bash
# Best for ESP32 (16GB RAM+)
ollama pull deepseek-coder-v2:16b

# Great alternative (14GB RAM+)
ollama pull qwen2.5-coder:14b

# Fast and efficient (8GB RAM)
ollama pull codellama:13b

# Lightweight option (6GB RAM)
ollama pull codellama:7b

# General purpose (8GB RAM)
ollama pull llama3.1:8b
```

### Test It

```bash
# Quick test
ollama run deepseek-coder-v2:16b "Write a simple ESP32 GPIO blink code"

# Interactive mode
ollama run qwen2.5-coder:14b
>>> Fix this ESP32 code: #include <stdio.h> void app_main() { gpio_set_level(GPIO_NUM_2, 1); }
```

### Integration

```python
from langchain_community.llms import Ollama

# Use in Developer Agent
llm = Ollama(
    model="deepseek-coder-v2:16b",
    base_url="http://localhost:11434",
    temperature=0
)
```

**Pros**:
- ‚úÖ Dead simple setup
- ‚úÖ Automatic model management
- ‚úÖ Works on Apple Silicon (Metal acceleration)
- ‚úÖ REST API included
- ‚úÖ Low latency

**Cons**:
- ‚ö†Ô∏è Limited customization
- ‚ö†Ô∏è Can't run multiple models simultaneously

---

## üé® Option 2: LM Studio (GUI Interface)

**Best for**: Users who prefer GUI, experimentation

### Installation

1. Download from [https://lmstudio.ai/](https://lmstudio.ai/)
2. Install (macOS/Windows/Linux)
3. Open LM Studio

### Download Models

In LM Studio GUI:
```
Search ‚Üí "deepseek-coder-v2" ‚Üí Download
Search ‚Üí "qwen2.5-coder" ‚Üí Download
Search ‚Üí "codellama" ‚Üí Download
```

### Start Server

```
1. Go to "Server" tab
2. Select model: deepseek-coder-v2-16b-Q4_K_M
3. Click "Start Server"
4. Server runs on: http://localhost:1234
```

### Integration

```python
from langchain_openai import ChatOpenAI

# LM Studio uses OpenAI-compatible API
llm = ChatOpenAI(
    model="deepseek-coder-v2",
    base_url="http://localhost:1234/v1",
    api_key="lm-studio",  # Dummy key
    temperature=0
)
```

**Pros**:
- ‚úÖ Beautiful GUI
- ‚úÖ Easy model switching
- ‚úÖ Chat interface for testing
- ‚úÖ OpenAI-compatible API
- ‚úÖ Performance metrics visible

**Cons**:
- ‚ö†Ô∏è GUI overhead
- ‚ö†Ô∏è Less scriptable than Ollama

---

## ‚ö° Option 3: vLLM (Production Grade)

**Best for**: Production deployments, maximum performance

### Installation

```bash
pip install vllm

# For CUDA (NVIDIA GPUs)
pip install vllm

# For Apple Silicon
pip install vllm-nightly
```

### Run Server

```bash
# Start vLLM server
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/deepseek-coder-6.7b-instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --dtype auto
```

### Integration

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="deepseek-ai/deepseek-coder-6.7b-instruct",
    base_url="http://localhost:8000/v1",
    api_key="token-abc123",
    temperature=0
)
```

**Pros**:
- ‚úÖ Highest throughput
- ‚úÖ Optimized inference
- ‚úÖ Batching support
- ‚úÖ Production-ready

**Cons**:
- ‚ö†Ô∏è More complex setup
- ‚ö†Ô∏è Requires technical knowledge

---

## üîß System Requirements

### Minimum Specs

| Model | CPU | RAM | GPU | Storage |
|-------|-----|-----|-----|---------|
| CodeLlama 7B | 4 cores | 8GB | Optional | 5GB |
| Llama 3.1 8B | 4 cores | 10GB | Optional | 6GB |
| CodeLlama 13B | 8 cores | 16GB | Recommended | 8GB |
| Qwen2.5 14B | 8 cores | 18GB | Recommended | 10GB |
| DeepSeek V2 16B | 8+ cores | 20GB | Recommended | 12GB |

### Recommended Specs

- **CPU**: Apple M1/M2/M3 or Intel i7/i9 8+ cores
- **RAM**: 32GB (run multiple models, cache)
- **GPU**: 
  - Apple Silicon: Use Metal (automatic)
  - NVIDIA: RTX 3060+ (12GB VRAM)
  - AMD: Not recommended
- **Storage**: SSD with 50GB+ free

### Performance Estimates

| Model | Mac M2 (16GB) | Mac M3 Max (64GB) | RTX 4090 (24GB) |
|-------|---------------|-------------------|-----------------|
| CodeLlama 7B | ~30 tokens/s | ~50 tokens/s | ~80 tokens/s |
| Llama 3.1 8B | ~25 tokens/s | ~45 tokens/s | ~75 tokens/s |
| CodeLlama 13B | ~15 tokens/s | ~35 tokens/s | ~60 tokens/s |
| Qwen2.5 14B | ~12 tokens/s | ~30 tokens/s | ~55 tokens/s |
| DeepSeek 16B | ~10 tokens/s | ~25 tokens/s | ~50 tokens/s |

---

## üéØ Model Selection Guide

### For Your Hardware

**If you have 8GB RAM**:
```bash
ollama pull codellama:7b
# or
ollama pull llama3.1:8b
```

**If you have 16GB RAM**:
```bash
ollama pull codellama:13b
# or  
ollama pull qwen2.5-coder:7b
```

**If you have 32GB+ RAM** (Recommended):
```bash
ollama pull deepseek-coder-v2:16b
# or
ollama pull qwen2.5-coder:14b
```

### For Your Use Case

**Quick fixes, simple errors**:
- `codellama:7b` (fastest)
- `llama3.1:8b` (good reasoning)

**ESP32-specific code, embedded systems**:
- `qwen2.5-coder:14b` ‚≠ê (excellent with C/C++)
- `deepseek-coder-v2:16b` (best quality)

**Complex refactoring, architecture**:
- `deepseek-coder-v2:16b` ‚≠ê (most powerful)
- `qwen2.5-coder:14b` (great alternative)

---

## üîê Privacy & Benefits

### Why Local Models?

**Privacy**:
- ‚úÖ Code never leaves your machine
- ‚úÖ No API keys needed
- ‚úÖ HIPAA/compliance friendly
- ‚úÖ Work offline

**Cost**:
- ‚úÖ No per-token charges
- ‚úÖ Unlimited usage
- ‚úÖ One-time hardware investment

**Performance**:
- ‚úÖ Lower latency (no network)
- ‚úÖ No rate limits
- ‚úÖ Predictable response times

**Cons**:
- ‚ö†Ô∏è Requires powerful hardware
- ‚ö†Ô∏è Initial setup complexity
- ‚ö†Ô∏è Models take disk space
- ‚ö†Ô∏è Quality depends on model size

---

## üß™ Testing Your Setup

### 1. Basic Test

```bash
# Terminal
ollama run deepseek-coder-v2:16b "Explain what this ESP32 code does: void app_main() { gpio_set_level(2, 1); }"
```

### 2. Python Test

```python
from langchain_community.llms import Ollama

llm = Ollama(model="deepseek-coder-v2:16b")

# Test prompt
response = llm.invoke("""
Fix this ESP32 code. Add missing includes and fix compilation errors:

void app_main() {
    gpio_set_level(GPIO_NUM_2, 1);
}
""")

print(response)
```

### 3. ESP32-Specific Test

```python
# Test with real ESP32 error
error_log = """
main.c:5:5: error: implicit declaration of function 'gpio_set_level'
main.c:5:5: error: 'GPIO_NUM_2' undeclared
"""

prompt = f"""
You are an ESP32 expert. Fix this compilation error:

Error:
{error_log}

Original code:
```c
#include <stdio.h>

void app_main() {{
    gpio_set_level(GPIO_NUM_2, 1);
}}
```

Provide the corrected code with all necessary includes.
"""

response = llm.invoke(prompt)
print(response)
```

---

## üìä Benchmarks

### Code Quality Comparison

| Task | CodeLlama 7B | Llama 3.1 8B | CodeLlama 13B | Qwen2.5 14B | DeepSeek 16B |
|------|--------------|--------------|---------------|-------------|--------------|
| Simple fixes | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| ESP32 specific | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Complex debug | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| C/C++ expertise | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Speed | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |

### Real-World Test Results

**Test**: Fix missing `#include "driver/gpio.h"`

| Model | Success Rate | Avg Time |
|-------|--------------|----------|
| CodeLlama 7B | 75% | 2.5s |
| Llama 3.1 8B | 80% | 2.8s |
| CodeLlama 13B | 90% | 4.2s |
| Qwen2.5-Coder 14B | 95% | 4.5s |
| DeepSeek-Coder-V2 16B | 98% | 5.8s |

---

## üéì Pro Tips

### 1. Model Quantization

Use quantized models for better performance:

```bash
# Q4_K_M = 4-bit quantization, good balance
ollama pull deepseek-coder-v2:16b-q4_K_M

# Q5_K_M = 5-bit, better quality
ollama pull deepseek-coder-v2:16b-q5_K_M

# Q8_0 = 8-bit, best quality (slower)
ollama pull deepseek-coder-v2:16b-q8_0
```

**Recommendation**: Use Q4_K_M for most cases.

### 2. Context Length

Configure context window:

```python
llm = Ollama(
    model="deepseek-coder-v2:16b",
    num_ctx=8192,  # Longer context for complex files
)
```

### 3. Temperature Settings

```python
# For code fixes (deterministic)
temperature=0

# For suggestions (creative)
temperature=0.3

# For exploration (diverse)
temperature=0.7
```

### 4. Caching

Enable prompt caching for faster responses:

```python
llm = Ollama(
    model="deepseek-coder-v2:16b",
    cache=True  # Cache repeated prompts
)
```

---

## üîÑ Fallback Strategy

Use local models with cloud backup:

```python
from langchain_community.llms import Ollama
from langchain_openai import ChatOpenAI

def get_llm(prefer_local=True):
    if prefer_local:
        try:
            # Try local first
            llm = Ollama(model="deepseek-coder-v2:16b")
            llm.invoke("test")  # Verify it works
            return llm
        except Exception as e:
            print(f"Local model failed: {e}")
            print("Falling back to OpenAI...")
    
    # Fallback to cloud
    return ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0
    )
```

---

## üìö Additional Resources

- **Ollama**: https://ollama.com/
- **LM Studio**: https://lmstudio.ai/
- **DeepSeek Coder**: https://github.com/deepseek-ai/DeepSeek-Coder
- **Qwen2.5-Coder**: https://github.com/QwenLM/Qwen2.5-Coder
- **CodeLlama**: https://github.com/facebookresearch/codellama

---

## üÜò Troubleshooting

### Ollama not responding

```bash
# Check if running
ps aux | grep ollama

# Restart
pkill ollama
ollama serve
```

### Out of memory

```bash
# Use smaller model
ollama pull codellama:7b

# Or reduce context
ollama run deepseek-coder-v2:16b --num-ctx 4096
```

### Slow responses

```bash
# Use quantized model
ollama pull deepseek-coder-v2:16b-q4_K_M

# Check CPU usage
top
```

### Model download fails

```bash
# Clear cache
rm -rf ~/.ollama/models/*

# Re-download
ollama pull deepseek-coder-v2:16b
```

---

## ‚úÖ Next Steps

1. ‚úÖ Install Ollama: `brew install ollama`
2. ‚úÖ Download model: `ollama pull qwen2.5-coder:14b`
3. ‚úÖ Test it: `ollama run qwen2.5-coder:14b`
4. ‚úÖ Integrate with Developer Agent (see main README)
5. ‚úÖ Optimize prompts for ESP32 development

**Ready to start Phase 2!** üöÄ
