# üåê ESP32 Dashboard - LLM Integration Guide

El dashboard ahora incluye integraci√≥n completa con LLM para desarrollo asistido por IA.

## üéØ Funcionalidades Nuevas

### 1. **üí¨ Chat con LLM** 
Interfaz de chat interactivo para hacer preguntas sobre ESP32.

**Caracter√≠sticas:**
- Respuestas en tiempo real
- Historial de conversaci√≥n
- Control de "temperatura" (creatividad)
- Preguntas r√°pidas predefinidas
- Formateo de c√≥digo en las respuestas

**Ejemplos de uso:**
- "How do I initialize WiFi on ESP32?"
- "Show me how to use GPIO interrupts"
- "Explain FreeRTOS tasks"
- "What are the best practices for power management?"

### 2. **‚ö° Generador de C√≥digo**
Genera c√≥digo ESP32 completo desde descripciones en lenguaje natural.

**Caracter√≠sticas:**
- Genera c√≥digo production-ready
- Incluye headers necesarios
- Comentarios explicativos
- Explicaci√≥n detallada del c√≥digo
- Copia con un click

**Ejemplo:**
```
Input: "Create a web server that controls 2 LEDs via HTTP API"
Output: C√≥digo completo con:
- Configuraci√≥n WiFi
- HTTP server setup
- Endpoints para cada LED
- Manejo de errores
```

### 3. **üîß Reparador de C√≥digo**
Fix autom√°tico de errores de compilaci√≥n usando IA.

**Caracter√≠sticas:**
- Diagn√≥stico del problema
- Fix autom√°tico del c√≥digo
- Lista de cambios realizados
- Nivel de confianza (high/medium/low)
- C√≥digo reparado listo para copiar

**Ejemplo:**
```
Error: implicit declaration of function 'gpio_set_direction'
Fix: Agrega #include "driver/gpio.h"
Confidence: high
```

## üöÄ C√≥mo Usar

### Acceso al Dashboard

```bash
# El dashboard est√° en:
http://localhost:8000

# Tabs disponibles:
üìä Dashboard - Monitoreo en tiempo real
üí¨ LLM Chat - Chat con el asistente
‚ö° Code Generator - Genera c√≥digo
üîß Code Fixer - Repara errores
```

### Uso del Chat

1. **Click en "üí¨ LLM Chat"**
2. **Escribe tu pregunta** en el campo de texto
3. **Presiona Enter o click en "Send"**
4. **El LLM responde** en tiempo real

**Atajos:**
- `Enter` - Enviar mensaje
- `Shift + Enter` - Nueva l√≠nea
- Botones de acciones r√°pidas en el sidebar

### Uso del Generador

1. **Click en "‚ö° Code Generator"**
2. **Describe** lo que quieres construir
3. **Selecciona** lenguaje y framework
4. **Click en "Generate Code"**
5. **Copia** el c√≥digo generado

**Tips:**
- S√© espec√≠fico en la descripci√≥n
- Menciona pines GPIO si es relevante
- Indica protocolos (I2C, SPI, UART, etc.)
- Especifica target ESP32 (S2, S3, C3, etc.)

### Uso del Code Fixer

1. **Click en "üîß Code Fixer"**
2. **Pega** tu c√≥digo con errores
3. **Pega** el mensaje de error
4. **Especifica** filename y component
5. **Click en "Fix Code"**
6. **Revisa** el diagn√≥stico y cambios
7. **Copia** el c√≥digo reparado

## üîß API Endpoints

Todos los endpoints est√°n disponibles en `/api/llm`:

### GET `/api/llm/status`
Verifica estado del LLM

```bash
curl http://localhost:8000/api/llm/status
```

**Response:**
```json
{
  "available": true,
  "provider": "ollama",
  "model": "qwen2.5-coder:7b",
  "base_url": "http://ollama:11434"
}
```

### POST `/api/llm/chat`
Chat con el LLM

```bash
curl -X POST http://localhost:8000/api/llm/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "How do I use I2C on ESP32?",
    "temperature": 0.7
  }'
```

**Request:**
```json
{
  "message": "string",
  "temperature": 0.7,
  "system_prompt": "optional string"
}
```

**Response:**
```json
{
  "response": "string",
  "model": "qwen2.5-coder:7b",
  "timestamp": "2025-11-17T..."
}
```

### POST `/api/llm/generate`
Genera c√≥digo

```bash
curl -X POST http://localhost:8000/api/llm/generate \
  -H "Content-Type: application/json" \
  -d '{
    "description": "Blink LED on GPIO 2",
    "language": "c",
    "framework": "esp-idf",
    "temperature": 0.1
  }'
```

**Request:**
```json
{
  "description": "string",
  "language": "c",
  "framework": "esp-idf",
  "temperature": 0.1
}
```

**Response:**
```json
{
  "code": "string",
  "explanation": "string",
  "model": "qwen2.5-coder:7b",
  "timestamp": "2025-11-17T..."
}
```

### POST `/api/llm/fix`
Repara c√≥digo

```bash
curl -X POST http://localhost:8000/api/llm/fix \
  -H "Content-Type: application/json" \
  -d '{
    "buggy_code": "your code...",
    "error_message": "error message...",
    "filename": "main.c",
    "component": "main"
  }'
```

**Request:**
```json
{
  "buggy_code": "string",
  "error_message": "string",
  "filename": "main.c",
  "component": "main"
}
```

**Response:**
```json
{
  "success": true,
  "fixed_code": "string",
  "changes_made": ["list of changes"],
  "confidence": "high",
  "diagnosis": "string"
}
```

### POST `/api/llm/analyze`
Analiza c√≥digo

```bash
curl -X POST http://localhost:8000/api/llm/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "code": "your code...",
    "analysis_type": "general"
  }'
```

**Analysis types:**
- `general` - An√°lisis general
- `error` - An√°lisis de errores (requiere `error_message`)
- `optimization` - Sugerencias de optimizaci√≥n

## ‚öôÔ∏è Configuraci√≥n

### Variables de Entorno

El dashboard lee estas variables de `.env`:

```bash
# LLM Provider
LLM_PROVIDER=ollama
LLM_MODEL=qwen2.5-coder:7b

# Ollama URLs
OLLAMA_BASE_URL=http://localhost:11434        # Desde host
OLLAMA_INTERNAL_URL=http://ollama:11434       # Desde containers

# Generation Parameters
LLM_TEMPERATURE=0.1        # 0.0-1.0 (deterministic ‚Üí creative)
LLM_MAX_TOKENS=4096        # Max response length
```

### Ajustar Temperatura

**En la UI:**
- Chat: Slider en sidebar (0.0 - 1.0)
- Generator: Dropdown con presets
- Fixer: Usa valor fijo √≥ptimo (0.1)

**Gu√≠a de temperatura:**
- `0.0 - 0.2`: Muy determin√≠stico (c√≥digo)
- `0.3 - 0.5`: Balanceado (explicaciones)
- `0.6 - 0.8`: Creativo (brainstorming)
- `0.9 - 1.0`: Muy creativo (puede ser impredecible)

### Cambiar Modelo

```bash
# En .env
LLM_MODEL=deepseek-coder-v2:16b

# O descargar nuevo modelo
docker exec esp32-ollama ollama pull deepseek-coder-v2:16b

# Reiniciar dashboard
docker-compose restart web-dashboard
```

## üé® Personalizaci√≥n

### Agregar Preguntas R√°pidas

Edita `/web-server/static/index.html`:

```html
<button class="quick-action-btn" onclick="sendQuickQuestion('Your question')">
    üî• Your Label
</button>
```

### Modificar System Prompts

Edita `/web-server/api/routes/llm.py`:

```python
system_prompt = (
    "You are an expert ESP32 developer. "
    "Your custom instructions here..."
)
```

### Agregar Nuevas Funcionalidades

1. **Backend**: Agrega endpoint en `api/routes/llm.py`
2. **Frontend**: Agrega funci√≥n en `static/js/llm.js`
3. **UI**: Agrega elementos en `static/index.html`
4. **Estilos**: Agrega CSS en `static/css/llm.css`

## üìä Uso con Workflows

El LLM se integra autom√°ticamente con el Developer Agent:

```python
from agent.orchestrator import AgentOrchestrator

# El orquestador usa el LLM autom√°ticamente
orchestrator = AgentOrchestrator(
    langchain_tools=tools,
    llm_provider="ollama",
    llm_model="qwen2.5-coder:7b"
)

# Cuando QA detecta un error, Developer Agent lo arregla
result = await orchestrator.run_workflow()
# El LLM genera el fix autom√°ticamente
```

## üîç Debugging

### LLM No Responde

```bash
# Verificar estado
curl http://localhost:8000/api/llm/status

# Verificar logs del dashboard
docker logs esp32-web-dashboard --tail 50

# Verificar logs de Ollama
docker logs esp32-ollama --tail 50

# Reiniciar servicios
docker-compose restart ollama web-dashboard
```

### Respuestas Lentas

- Usa modelo m√°s peque√±o (7b en vez de 14b)
- Reduce `LLM_MAX_TOKENS`
- Baja temperatura para respuestas m√°s directas

### Errores de Conexi√≥n

```bash
# Desde el dashboard, debe llegar a ollama
docker exec esp32-web-dashboard curl http://ollama:11434/api/tags

# Si falla, verificar red
docker network inspect embedded-ia_esp32-network
```

## üìö Ejemplos

### Ejemplo 1: Generar C√≥digo de Sensor

**Input:**
```
Read temperature from DHT22 sensor on GPIO 4.
Print to serial every 2 seconds.
Handle sensor errors gracefully.
```

**Output:**
```c
#include <stdio.h>
#include "freertos/FreeRTOS.h"
#include "freertos/task.h"
#include "driver/gpio.h"
#include "dht.h"

#define DHT_GPIO GPIO_NUM_4

void app_main(void) {
    gpio_set_pull_mode(DHT_GPIO, GPIO_PULLUP_ONLY);
    
    while(1) {
        float temperature = 0, humidity = 0;
        
        if (dht_read_float_data(DHT_TYPE_DHT22, DHT_GPIO, 
                                 &humidity, &temperature) == ESP_OK) {
            printf("Temperature: %.1f¬∞C, Humidity: %.1f%%\n", 
                   temperature, humidity);
        } else {
            printf("Error reading DHT22 sensor\n");
        }
        
        vTaskDelay(pdMS_TO_TICKS(2000));
    }
}
```

### Ejemplo 2: Fix de Error de WiFi

**Buggy Code:**
```c
#include "esp_wifi.h"

void app_main() {
    wifi_init_config_t cfg = WIFI_INIT_CONFIG_DEFAULT();
    ESP_ERROR_CHECK(esp_wifi_init(&cfg));
}
```

**Error:**
```
undefined reference to `nvs_flash_init'
```

**Fixed Code:**
```c
#include "nvs_flash.h"
#include "esp_wifi.h"

void app_main() {
    // Initialize NVS (required for WiFi)
    ESP_ERROR_CHECK(nvs_flash_init());
    
    wifi_init_config_t cfg = WIFI_INIT_CONFIG_DEFAULT();
    ESP_ERROR_CHECK(esp_wifi_init(&cfg));
}
```

**Diagnosis:**
WiFi requires NVS (Non-Volatile Storage) to be initialized first for storing configuration.

## üéØ Best Practices

1. **S√© espec√≠fico** en tus preguntas y descripciones
2. **Incluye contexto** relevante (target ESP32, pines, protocolos)
3. **Verifica el c√≥digo** generado antes de usarlo en producci√≥n
4. **Usa temperatura baja** (0.1-0.3) para c√≥digo
5. **Usa temperatura alta** (0.6-0.8) para brainstorming
6. **Revisa fixes** del LLM antes de aplicarlos
7. **Combina** chat + generator para mejor resultado

## üÜò Soporte

- **Documentaci√≥n completa**: `docs/LLM_USAGE.md`
- **API Docs**: http://localhost:8000/docs
- **Issues**: GitHub Issues
- **Chat en tiempo real**: Tab "üí¨ LLM Chat" en el dashboard

## üéâ Pr√≥ximas Funcionalidades

- [ ] Historial de conversaciones persistente
- [ ] Templates de c√≥digo predefinidos
- [ ] An√°lisis de proyectos completos
- [ ] Sugerencias proactivas en logs
- [ ] Integraci√≥n con GitHub Copilot
- [ ] Exportar c√≥digo generado directo a archivos
- [ ] Comparaci√≥n de c√≥digo antes/despu√©s
- [ ] M√©tricas de uso del LLM
