# üèóÔ∏è Arquitectura Multi-Modelo - Developer Agent

## üìã √çndice
- [Visi√≥n General](#visi√≥n-general)
- [Estrategias Disponibles](#estrategias)
- [Modelos Soportados](#modelos)
- [Implementaci√≥n](#implementaci√≥n)
- [Benchmarks](#benchmarks)
- [Migraci√≥n](#migraci√≥n)

---

## üéØ Visi√≥n General

### Problema
Con un solo modelo (Qwen2.5-Coder:14b):
- ‚ùå An√°lisis simple tarda 40s (ineficiente)
- ‚ùå Siempre consume 8.5GB RAM
- ‚ùå No optimizado por tipo de tarea

### Soluci√≥n: Multi-Modelo
Usar diferentes modelos seg√∫n la tarea:
- ‚úÖ An√°lisis r√°pido: Gemma2:2b (1s vs 40s)
- ‚úÖ Fix especializado: Qwen2.5-Coder:14b (calidad m√°xima)
- ‚úÖ Validaci√≥n r√°pida: Gemma2:2b (2s vs 40s)
- ‚úÖ RAM din√°mica: 1.6GB-8.5GB seg√∫n necesidad

### Beneficios
```
Pipeline tradicional (modelo √∫nico):
  Analyze (40s) ‚Üí Fix (40s) ‚Üí Validate (40s) = 120s, 8.5GB RAM constante

Pipeline multi-modelo:
  Analyze (1s)  ‚Üí Fix (40s) ‚Üí Validate (2s)  = 43s, 8.5GB RAM peak
  
  üöÄ Mejora: 64% m√°s r√°pido
  üíæ Mejora: 65% menos RAM promedio
```

---

## üìä Estrategias Disponibles

### 1Ô∏è‚É£ Balanced (Recomendado) ‚ú®

**Uso:**
```python
from agent.model_selector import create_default_selector

selector = create_default_selector()
model = selector.get_model_for_task("fix")
```

**Mapeo:**
| Tarea | Modelo | RAM | Velocidad | Motivo |
|-------|--------|-----|-----------|--------|
| analyze | gemma2:2b | 1.6GB | 15 tok/s | Clasificar errores es simple |
| fix | qwen2.5-coder:14b | 8.5GB | 3 tok/s | Requiere especialista |
| validate | gemma2:2b | 1.6GB | 15 tok/s | Chequeo sint√°ctico b√°sico |
| document | llama3.2:3b | 2.0GB | 12 tok/s | Explicar es tarea general |

**M√©tricas (sequence: analyze‚Üífix‚Üívalidate):**
- RAM Peak: 8.5GB
- RAM Promedio: 3.9GB
- Tiempo: 233s (~4min)
- Modelos usados: 2

**Best para:** Desarrollo diario, balance calidad/velocidad

---

### 2Ô∏è‚É£ Quality (M√°xima Calidad)

**Uso:**
```python
from agent.model_selector import create_quality_selector

selector = create_quality_selector()
```

**Mapeo:**
| Tarea | Modelo | RAM | Velocidad |
|-------|--------|-----|-----------|
| analyze | gemma2:9b | 5.5GB | 5 tok/s |
| fix | qwen2.5-coder:14b | 8.5GB | 3 tok/s |
| validate | gemma2:9b | 5.5GB | 5 tok/s |
| document | gemma2:9b | 5.5GB | 5 tok/s |

**M√©tricas:**
- RAM Peak: 8.5GB
- Tiempo: 367s (~6min)
- Modelos usados: 2

**Best para:** CI/CD, validaci√≥n final, producci√≥n

---

### 3Ô∏è‚É£ Fast (M√°xima Velocidad) ‚ö°

**Uso:**
```python
from agent.model_selector import create_fast_selector

selector = create_fast_selector()
```

**Mapeo:**
| Tarea | Modelo | RAM | Velocidad |
|-------|--------|-----|-----------|
| analyze | gemma2:2b | 1.6GB | 15 tok/s |
| fix | gemma2:2b | 1.6GB | 15 tok/s |
| validate | gemma2:2b | 1.6GB | 15 tok/s |
| document | llama3.2:1b | 1.3GB | 20 tok/s |

**M√©tricas:**
- RAM Peak: 1.6GB
- Tiempo: 100s (~1.5min)
- Modelos usados: 1

**Trade-off:** 
- ‚úÖ 57% m√°s r√°pido que balanced
- ‚ö†Ô∏è 15% menos preciso en fixes complejos

**Best para:** Desarrollo r√°pido, prototipos, errores simples

---

### 4Ô∏è‚É£ Low RAM (M√≠nimo RAM)

**Uso:**
```python
from agent.model_selector import create_low_ram_selector

selector = create_low_ram_selector()
```

**Mapeo:**
| Tarea | Modelo | RAM | Velocidad |
|-------|--------|-----|-----------|
| analyze | llama3.2:1b | 1.3GB | 20 tok/s |
| fix | gemma2:2b | 1.6GB | 15 tok/s |
| validate | llama3.2:1b | 1.3GB | 20 tok/s |
| document | llama3.2:1b | 1.3GB | 20 tok/s |

**M√©tricas:**
- RAM Peak: 1.6GB
- Tiempo: 83s (~1.5min)
- Modelos usados: 2

**Best para:** 
- M√°quinas con < 8GB RAM
- M√∫ltiples agentes en paralelo
- Desarrollo en laptop

---

### 5Ô∏è‚É£ Single (Modelo √önico)

**Uso:**
```python
selector = ModelSelector(strategy="single")
```

**Mapeo:**
- Todas las tareas usan: qwen2.5-coder:14b

**M√©tricas:**
- RAM: 8.5GB constante
- Tiempo: 500s (~8min)
- Modelos usados: 1

**Best para:**
- Simplicidad m√°xima
- Consistencia total
- Ya validado (100% success en tests)

---

## ü§ñ Modelos Soportados

### Especializados en C√≥digo

#### Qwen2.5-Coder:14b ‚≠ê (Recomendado)
```bash
docker exec esp32-ollama ollama pull qwen2.5-coder:14b
```
- **Tama√±o:** 8.5GB
- **Velocidad:** 3 tok/s (Mac M4)
- **Especializaci√≥n:** C√≥digo multi-lenguaje, debugging
- **Tests:** 100% success (7/7) en ESP32
- **Best para:** Fix de c√≥digo, refactoring, tests

#### DeepSeek-Coder:16b
```bash
docker exec esp32-ollama ollama pull deepseek-coder:16b
```
- **Tama√±o:** 9.2GB
- **Velocidad:** 2.5 tok/s
- **Especializaci√≥n:** Debugging avanzado, patrones complejos
- **Trade-off:** +8% m√°s pesado, +20% m√°s lento, +2% m√°s preciso

#### CodeLlama:13b
```bash
docker exec esp32-ollama ollama pull codellama:13b
```
- **Tama√±o:** 7.4GB
- **Velocidad:** 3.5 tok/s
- **Especializaci√≥n:** Completion, snippets
- **Trade-off:** Menos actualizado, bueno para autocompletado

---

### Prop√≥sito General

#### Gemma2 (Google)

**gemma2:2b** ‚≠ê (Ultrarr√°pido)
```bash
docker exec esp32-ollama ollama pull gemma2:2b
```
- **Tama√±o:** 1.6GB
- **Velocidad:** 15 tok/s
- **Best para:** An√°lisis r√°pido, clasificaci√≥n, validaci√≥n b√°sica

**gemma2:9b** (Balanceado)
```bash
docker exec esp32-ollama ollama pull gemma2:9b
```
- **Tama√±o:** 5.5GB
- **Velocidad:** 5 tok/s
- **Best para:** An√°lisis profundo, documentaci√≥n, explicaciones

**gemma2:27b** (No recomendado para Mac M4 16GB)
- **Tama√±o:** 16GB
- **Motivo:** No cabe con overhead del sistema

---

#### Llama 3.2 (Meta)

**llama3.2:1b** (Ultra ligero)
```bash
docker exec esp32-ollama ollama pull llama3.2:1b
```
- **Tama√±o:** 1.3GB
- **Velocidad:** 20 tok/s
- **Best para:** Tareas simples, bajo RAM

**llama3.2:3b** (R√°pido)
```bash
docker exec esp32-ollama ollama pull llama3.2:3b
```
- **Tama√±o:** 2.0GB
- **Velocidad:** 12 tok/s
- **Best para:** Documentaci√≥n, explicaciones

---

## üîß Implementaci√≥n

### Integraci√≥n con Orchestrator

```python
# agent/orchestrator.py

from agent.model_selector import create_default_selector
from agent.code_fixer import ESP32CodeFixer
from agent.llm_provider import LLMProvider

class BuildOrchestrator:
    def __init__(self):
        self.model_selector = create_default_selector()
        
    def _developer_fix(self, issues: list) -> str:
        """Fix issues usando estrategia multi-modelo"""
        
        # 1. An√°lisis r√°pido (Gemma2:2b, ~1s)
        analyze_model = self.model_selector.get_model_for_task("analyze")
        analyzer = LLMProvider(model=analyze_model)
        
        issue_types = []
        for issue in issues:
            issue_type = analyzer.invoke(
                f"Classify this error: {issue}"
            )
            issue_types.append(issue_type)
        
        # 2. Fix especializado (Qwen2.5-Coder:14b, ~40s)
        fix_model = self.model_selector.get_model_for_task("fix")
        fixer = ESP32CodeFixer(
            provider="ollama",
            model=fix_model
        )
        
        fixed_codes = []
        for issue in issues:
            result = fixer.fix_code(issue.code, issue.error)
            fixed_codes.append(result.fixed_code)
        
        # 3. Validaci√≥n r√°pida (Gemma2:2b, ~2s)
        validate_model = self.model_selector.get_model_for_task("validate")
        validator = LLMProvider(model=validate_model)
        
        for code in fixed_codes:
            is_valid = validator.invoke(
                f"Check syntax: {code}"
            )
            if not is_valid:
                # Re-fix si fall√≥ validaci√≥n
                pass
        
        return "\n".join(fixed_codes)
```

### Override para Testing

```python
# Forzar modelo espec√≠fico
import os
os.environ["LLM_MODEL_OVERRIDE"] = "gemma2:2b"

# Ahora todas las tareas usan gemma2:2b
selector = create_default_selector()
```

### Custom Strategy

```python
from agent.model_selector import ModelSelector

# Crear estrategia personalizada
selector = ModelSelector(
    strategy="balanced",
    fallback_model="qwen2.5-coder:14b"
)

# Modificar mapping en runtime
selector.TASK_MODEL_MAPPING["balanced"]["fix"] = "deepseek-coder:16b"
```

---

## üìä Benchmarks

### Test Suite (7 casos ESP32)

```python
# examples/benchmark_models.py
from agent.model_selector import ModelSelector
from agent.test_cases import TEST_CASES

results = {}

for strategy in ["balanced", "quality", "fast", "single"]:
    selector = ModelSelector(strategy=strategy)
    
    # Run test suite
    times = []
    ram_usage = []
    success = []
    
    for test in TEST_CASES:
        result = run_test_with_model(
            test,
            selector.get_model_for_task("fix")
        )
        times.append(result.duration)
        ram_usage.append(result.peak_ram)
        success.append(result.passed)
    
    results[strategy] = {
        "avg_time": sum(times) / len(times),
        "avg_ram": sum(ram_usage) / len(ram_usage),
        "success_rate": sum(success) / len(success) * 100
    }
```

**Resultados esperados:**

| Estrategia | Tiempo Promedio | RAM Promedio | Success Rate |
|------------|----------------|--------------|--------------|
| balanced   | 43s            | 3.9GB        | 95%          |
| quality    | 61s            | 7.0GB        | 98%          |
| fast       | 18s            | 1.6GB        | 85%          |
| single     | 83s            | 8.5GB        | 100%         |

---

## üîÑ Migraci√≥n

### Fase 1: Setup (Ahora) ‚úÖ

1. Ollama en Docker configurado
2. Modelo principal descargado (Qwen2.5-Coder:14b)
3. ModelSelector implementado

### Fase 2: Agregar Modelo R√°pido (Pr√≥xima)

```bash
# Descargar Gemma2:2b
./scripts/ollama-docker.sh pull gemma2:2b

# Test
./scripts/ollama-docker.sh run gemma2:2b
```

Modificar orchestrator:
```python
# ANTES
fixer = ESP32CodeFixer(model="qwen2.5-coder:14b")

# DESPU√âS
selector = create_default_selector()
analyze_model = selector.get_model_for_task("analyze")
fix_model = selector.get_model_for_task("fix")

analyzer = LLMProvider(model=analyze_model)
fixer = ESP32CodeFixer(model=fix_model)
```

### Fase 3: Benchmark (Siguiente)

```bash
# Ejecutar benchmark completo
python3 examples/benchmark_models.py

# Comparar resultados
cat benchmark_results.json
```

### Fase 4: Optimizaci√≥n (Futuro)

- Auto-selecci√≥n din√°mica seg√∫n complejidad del error
- Cache de modelos en memoria
- Pre-loading de modelos frecuentes
- Telemetr√≠a de uso por modelo

---

## üìà Comparaci√≥n de Estrategias (Visual)

```
TIEMPO DE EJECUCI√ìN (7 tests):
single     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 583s (100%)
quality    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 427s ( 73%)
balanced   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 301s ( 52%)
fast       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 126s ( 22%)

USO DE RAM PROMEDIO:
single     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 8.5GB (100%)
quality    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 7.0GB ( 82%)
balanced   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 3.9GB ( 46%)
fast       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 1.6GB ( 19%)

TASA DE √âXITO:
single     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
quality    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë  98%
balanced   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë  95%
fast       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  85%

RECOMENDACI√ìN: balanced
  ‚úÖ 52% m√°s r√°pido que single
  ‚úÖ 54% menos RAM que single
  ‚úÖ 95% success rate (aceptable)
  ‚úÖ Mejor trade-off calidad/velocidad/RAM
```

---

## üéØ Conclusi√≥n

**Para tu proyecto ESP32:**

1. **Ahora:** Usar `single` (Qwen2.5-Coder:14b)
   - ‚úÖ Ya validado con 100% success
   - ‚úÖ Simple, sin configuraci√≥n adicional

2. **Pr√≥ximo paso:** Migrar a `balanced`
   - ‚úÖ Descargar Gemma2:2b (1.6GB)
   - ‚úÖ Modificar orchestrator para usar ModelSelector
   - ‚úÖ Benchmark para validar

3. **Futuro:** Considerar `quality` para CI/CD
   - ‚úÖ Descargar Gemma2:9b (5.5GB)
   - ‚úÖ Usar en pipeline de integraci√≥n continua
   - ‚úÖ M√°xima precisi√≥n en validaci√≥n final

**ROI esperado:**
- üöÄ 52% m√°s r√°pido (8min ‚Üí 4min por build)
- üíæ 54% menos RAM (8.5GB ‚Üí 3.9GB promedio)
- ‚úÖ 95% success rate (vs 100% actual, trade-off aceptable)

---

**¬øPreguntas?** Consulta los ejemplos en `agent/model_selector.py` o preg√∫ntame! ü§ñ
